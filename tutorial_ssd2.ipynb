{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning implementation\n",
    "\n",
    "Single Shot Detector based Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install efficientnet\n",
    "!wget -P checkpoints https://github.com/500swapnil/Keras_Efficientnet_SSD/releases/download/v1.0/efficientnetb0_SSD.h5\n",
    "!wget -P checkpoints https://github.com/500swapnil/Keras_Efficientnet_SSD/releases/download/v1.0/efficientnetb5_SSD.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Shot Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load python packet   \n",
    "# ----------------------start--------------------------------\n",
    "# load necessary packets\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "import tensorflow as tf\n",
    "\n",
    "# load utilities\n",
    "from utils.priors import *\n",
    "from model.ssd import ssd\n",
    "from utils.post_processing import post_process\n",
    "from utils.preprocessing import prepare_for_prediction\n",
    "# -------------------------end-------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters & directory paths\n",
    "# ----------------------start--------------------------------\n",
    "# define parameter\n",
    "IMAGE_SIZE = [300, 300]\n",
    "# define parameter\n",
    "BATCH_SIZE = 16\n",
    "# choose backbone net [B0, B1, B2, ..., B7]\n",
    "MODEL_NAME = 'B0'\n",
    "\n",
    "# directory path of weight of pretrained model [./checkpoints/efficientnetb0_SSD.h5]\n",
    "checkpoint_filepath = ??? \n",
    "# directory path of input image\n",
    "INPUT_DIR = './images'\n",
    "# directory path of output image\n",
    "OUTPUT_DIR = './outputs'\n",
    "# -------------------------end-------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters of SSD\n",
    "# ----------------------start--------------------------------\n",
    "# set positive box's threshold\n",
    "iou_threshold = 0.5\n",
    "# set anchor center's variance\n",
    "center_variance = 0.1\n",
    "# set anchor size's variance\n",
    "size_variance = 0.2\n",
    "\n",
    "# set anchor boxes\n",
    "specs = [\n",
    "                SSDSpec(38, 8, SSDBoxSizes(30, 60), [2]),\n",
    "                SSDSpec(19, 16, SSDBoxSizes(60, 111), [2, 3]),\n",
    "                SSDSpec(10, 32, SSDBoxSizes(111, 162), [2, 3]),\n",
    "                SSDSpec(5, 64, SSDBoxSizes(162, 213), [2, 3]),\n",
    "                SSDSpec(3, 100, SSDBoxSizes(213, 264), [2]),\n",
    "                SSDSpec(1, 300, SSDBoxSizes(264, 315), [2])\n",
    "        ]\n",
    "\n",
    "# create SSD's anchor boxes\n",
    "priors = generate_ssd_priors(specs, IMAGE_SIZE[0])\n",
    "target_transform = MatchPrior(priors, center_variance, size_variance, iou_threshold)\n",
    "# -------------------------end-------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build SSD & load checkpoint\n",
    "# ----------------------start--------------------------------\n",
    "# build SSD\n",
    "print(\"Building SSD Model with EfficientNet{0} backbone..\".format(MODEL_NAME))\n",
    "model = ssd(MODEL_NAME, pretrained=False)\n",
    "\n",
    "# load checkpoint\n",
    "print(\"Loading Checkpoint..\")\n",
    "if checkpoint_filepath is not None:\n",
    "    print(\"Loading Checkpoint..\", checkpoint_filepath)\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "else:\n",
    "    print(\"Training from with only base model pretrained on imagenet\")\n",
    "# -------------------------end-------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image\n",
    "# ----------------------start--------------------------------\n",
    "dataset = tf.data.Dataset.list_files(INPUT_DIR + '/*', shuffle=False)\n",
    "filenames = list(dataset.as_numpy_iterator())\n",
    "dataset = dataset.map(prepare_for_prediction)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "# -------------------------end-------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the image\n",
    "# ----------------------start--------------------------------\n",
    "pred = model.predict(dataset, verbose=1)\n",
    "predictions = post_process(pred, target_transform, confidence_threshold=0.4)\n",
    "# -------------------------end-------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw boxes of human & save image\n",
    "# ----------------------start--------------------------------\n",
    "print(\"Prediction Complete\")\n",
    "for i, path in enumerate(filenames):\n",
    "    path_string = path.decode(\"utf-8\")\n",
    "    im = imread(path_string)\n",
    "    filename = path_string.split('/')[-1]\n",
    "    fig, ax = plt.subplots(1, figsize=(15, 15))\n",
    "    ax.imshow(im)\n",
    "    pred_boxes, pred_scores, pred_labels = predictions[i]\n",
    "    if pred_boxes.size > 0:\n",
    "        draw_bboxes(pred_boxes, ax , labels=pred_labels, IMAGE_SIZE=im.shape[:2])\n",
    "    plt.axis('off')\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'out_'+ filename), bbox_inches='tight', pad_inches=0)\n",
    "    \n",
    "print(\"Output is saved in\", OUTPUT_DIR)\n",
    "# -------------------------end-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### try to predict without training\n",
    "\n",
    "```bash\n",
    "# predict with new model\n",
    "!python predict_ssd.py\n",
    "\n",
    "# predict with final model\n",
    "!python predict_ssd.py --checkpoint ./checkpoints/efficientnetb0_SSD.h5\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python predict_ssd.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### try to predict with training weigth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python predict_ssd.py --checkpoint ./checkpoints/efficientnetb0_SSD.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load python packet   \n",
    "# ----------------------start--------------------------------\n",
    "# load necessary packets\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# load utilities\n",
    "from utils.priors import *\n",
    "from model.ssd import ssd\n",
    "from model.loss import multibox_loss\n",
    "from utils.preprocessing import prepare_dataset\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler,  ReduceLROnPlateau, ModelCheckpoint\n",
    "# -------------------------end-------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters & directory paths\n",
    "# ----------------------start--------------------------------\n",
    "# define directory path of dataset\n",
    "DATASET_DIR = './dataset'\n",
    "# define parameter\n",
    "IMAGE_SIZE = [300, 300]\n",
    "# define parameter\n",
    "BATCH_SIZE = 16\n",
    "# choose backbone net [B0, B1, B2, ..., B7]\n",
    "MODEL_NAME = 'B0'\n",
    "# define number of epochs\n",
    "EPOCHS = 5\n",
    "\n",
    "# directory path of weight of pretrained model [./checkpoints/efficientnetb0_SSD.h5]\n",
    "checkpoint_filepath = ??? \n",
    "# -------------------------end-------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build data loader\n",
    "# ----------------------start--------------------------------\n",
    "print(\"Loading Data..\")\n",
    "train_data = tfds.load(\"voc\", data_dir=DATASET_DIR, split='train')\n",
    "number_train = train_data.reduce(0, lambda x, _: x + 1).numpy()\n",
    "print(\"Number of Training Files:\", number_train)\n",
    "# -------------------------end-------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters of SSD\n",
    "# ----------------------start--------------------------------\n",
    "# set positive box's threshold\n",
    "iou_threshold = 0.5\n",
    "# set anchor center's variance\n",
    "center_variance = 0.1\n",
    "# set anchor size's variance\n",
    "size_variance = 0.2\n",
    "\n",
    "# set anchor boxes\n",
    "specs = [\n",
    "                SSDSpec(38, 8, SSDBoxSizes(30, 60), [2]),\n",
    "                SSDSpec(19, 16, SSDBoxSizes(60, 111), [2, 3]),\n",
    "                SSDSpec(10, 32, SSDBoxSizes(111, 162), [2, 3]),\n",
    "                SSDSpec(5, 64, SSDBoxSizes(162, 213), [2, 3]),\n",
    "                SSDSpec(3, 100, SSDBoxSizes(213, 264), [2]),\n",
    "                SSDSpec(1, 300, SSDBoxSizes(264, 315), [2])\n",
    "        ]\n",
    "\n",
    "# create SSD's anchor boxes\n",
    "priors = generate_ssd_priors(specs, IMAGE_SIZE[0])\n",
    "target_transform = MatchPrior(priors, center_variance, size_variance, iou_threshold)\n",
    "# instantiate the datasets\n",
    "training_dataset = prepare_dataset(train_data, IMAGE_SIZE, BATCH_SIZE, target_transform, train=True)\n",
    "# -------------------------end-------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build SSD & calculate number of steps\n",
    "# ----------------------start--------------------------------\n",
    "# build SSD\n",
    "print(\"Building SSD Model with EfficientNet{0} backbone..\".format(MODEL_NAME))\n",
    "model = ssd(MODEL_NAME)\n",
    "# calculate number of steps\n",
    "steps_per_epoch = number_train // BATCH_SIZE\n",
    "print(\"Number of Train Batches:\", steps_per_epoch)\n",
    "# method summarizes detail of model\n",
    "model.summary()\n",
    "# -------------------------end-------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters of model\n",
    "# ----------------------start--------------------------------\n",
    "# define learning rate scheduler\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=1, min_lr=1e-5, verbose=1)\n",
    "# define checkpoint method\n",
    "checkpoint = ModelCheckpoint(checkpoint_filepath, monitor='val_loss', save_best_only=True, save_weights_only=True, verbose=1)\n",
    "# set initial learning rate\n",
    "base_lr = 1e-3 if checkpoint_filepath is None else 1e-5\n",
    "# -------------------------end-------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define compiler\n",
    "# ----------------------start--------------------------------\n",
    "model.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=base_lr),\n",
    "    loss = multibox_loss\n",
    ")\n",
    "# -------------------------end-------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resume checkpoint\n",
    "# ----------------------start--------------------------------\n",
    "if checkpoint_filepath is not None:\n",
    "    print(\"Continuing Training from\", checkpoint_filepath)\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "else:\n",
    "    print(\"Training from with only base model pretrained on imagenet\")\n",
    "# -------------------------end-------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "# ----------------------start--------------------------------\n",
    "history = model.fit(training_dataset, \n",
    "                    steps_per_epoch=steps_per_epoch, \n",
    "                    epochs=EPOCHS,\n",
    "                    callbacks=[reduce_lr,checkpoint]) \n",
    "# -------------------------end-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inference method\n",
    "```bash\n",
    "# run inference method with new model\n",
    "!python inference_ssd.py -debug -fill --checkpoint None\n",
    "\n",
    "# run inference method with final model\n",
    "!python inference_ssd.py -debug -fill --checkpoint ./checkpoints/efficientnetb0_SSD.h5\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
